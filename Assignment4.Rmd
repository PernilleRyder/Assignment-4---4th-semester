
---
title: "Assignment 4 - Applying meta-analytic priors"
author: "Riccardo Fusaroli"
date: "3/16/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 4

In this assignment we do the following:
- we reproduce the meta-analysis of pitch SD from last semester in a Bayesian framework
- we reproduce the pitch SD in schizophrenia analysis from last semester using both a conservative and a meta-analytic prior
- we assess the difference in model quality and estimates using the two priors.

The questions you need to answer are: What are the consequences of using a meta-analytic prior? Evaluate the models with conservative and meta-analytic priors. Discuss the effects on estimates. Discuss the effects on model quality. Discuss the role that meta-analytic priors should have in scientific practice. Should we systematically use them? Do they have drawbacks? Should we use them to complement more conservative approaches? How does the use of meta-analytic priors you suggest reflect the skeptical and cumulative nature of science?

### Step by step suggestions

Step 1: Reproduce the meta-analysis of pitch sd from previous studies of voice in schizophrenia
- the data is available as Assignment4MetaData.xlsx
- Effect size (cohen's d), sd and variance are already calculated (you're welcome!)
- Since we're only interested in getting a meta-analytic effect size, let's take a shortcut and use bromance magic (brms): https://mvuorre.github.io/post/2016/2016-09-29-bayesian-meta-analysis/

```{r}
library(readxl)
md <- read_excel("~/Desktop/Computational models/Assignment 4/Assignment4MetaData.xlsx")

library(rstan)
library(brms)
library(tidyverse)

m = brm(MeanES | se(SdES)~ 1 + (1|StudyRef), data = md, cores = 2, chain =2, iter = 2000)

summary(m)

#--> MeanES has an uncertainty(se), find it in SdES. 
#cores, how many processers the data should run on. It is a big data set, 2
#chain, 2
#iteration, how long should each models search for a solution before stopping, 2000
#use the default prior within the brms package

devtools::install_github("mvuorre/brmstools")
library(brmstools)
forest(m, show_data = TRUE,
       av_name = "Effect size")

#extract meta-analytic prior = estimates and error
#population-level effects
#Estimate: -0.55
#Estimate error: 0.26
#sd of intercept: 0.72
#se of sd of intercept: 0.25 
#meta-analytic prior should look gaussian
```

Step 2: Prepare the pitch SD data from last year
- the data is available as Assignment4PitchData.csv (thanks Celine)
- We do not know how to build random effects, yet. So we need to simplify the dataset to avoid interdependence between datapoint: How?
- Also, let's standardize the data, so that they are compatible with our meta-analytic prior (Cohen's d is measured in SDs).

```{r}
#Doesn't work

# Eliminate the need to use random effects, how to do? One datapoint per participant
# Mean within each participants, mean of data points

# #One pitchmean data point per participant 
# ag_mean = aggregate(PitchMean ~ ID, pd, mean)
# 
# #One pitchsd data point per participant
# ag_sd = aggregate(PitchSD~ ID, pd, mean)
# 
# #merge the two subsets
# ag_data = merge(ag_sd, ag_mean, by="ID")

# #rename columns
# ag_data = plyr::rename(ag_data, c("PitchMean" = "PitchMeanMean", "PitchSD" = "PitchSDMean"))
# 
# #merge aggregatted data with pd data
# pd_mean = merge(pd, ag_data, by = "ID")
# 
# #standardize the pitchsd
# pd_mean$PitchSDMean.c = pd_mean$PitchSDMean - mean(pd_mean$PitchSDMean) / sd(pd_mean$PitchSDMean)

#This works
pd <- read_excel("~/Desktop/Computational models/Assignment 4/Assignment4PitchDatav2.xlsx")

# Taking the mean of all values in the pitch mean data frame, split by participant
mean = aggregate(pd, by = list(pd$ID_unique), FUN = mean)

# subsetting pitchmean, pitchsd, and ID, so we can put it into the pitch data frame
subset = subset(mean, select = c(ID_unique,diagnosis,PitchMean, PitchSD))

# renaming the variables
subset = plyr::rename(subset, c("PitchMean"="PitchMean.m", "PitchSD"="PitchSD.m"))

#standardizing the standard deviation
subset$PitchSD.m.c = (subset$PitchSD.m-mean(subset$PitchSD.m))/sd(subset$PitchSD.m)

```

Step 3: Build a regression model predicting Pitch SD from Diagnosis.
- how is the outcome distributed? (likelihood function)
- how are the parameters of the likelihood distribution distributed? Which predictors should they be conditioned on?
- use a skeptical/conservative prior for the effects of diagnosis. Remember you'll need to motivate it.
- Describe and plot the estimates. Evaluate model quality

Why is the distribuution Gaussian?
- The outcome is continous

Sampling form the mean of participants
The means of random sampling has a gaussian dstribution

We want to standardize SD, with a mean of zero and sd of 1 = how we construct alpha
Which kind of distribution is beta? (telling the difference between schizo and control)
- gaussian going from -3 to 3, a continous variable 

Sigma, how much do we expect the model to vary, error in the model. 
Can only be positive

Conservative prior - beta(0, .2)
```{r}
library(rethinking)

#model with a skeptical 
m2 <- map2stan(
  alist(
    PitchSD.m.c ~ dnorm(mu, sigma),
    mu <- alpha + beta * diagnosis,
    alpha ~ dnorm (0,1), #there is no difference between the diagnosis then the intercept will be one
    beta ~ dnorm (0,0.2),
    sigma ~ dcauchy(0,2)
  ), 
  data=subset)
precis(m2)

#dens plot on simulated data
#Simulating data 
sim.m2 = sim(m2, data = subset, n = 1e4)

#dens plot simulated data from model on raw data
dens(sim.m2, col = "red", xlim = c(-5, 5), ylim = c(0,1),)
par(new=TRUE)
dens(subset$PitchSD.m.c, xlim = c(-5, 5), ylim = c(0,1),)
title("Model 2")


```

Step 4: Now re-run the model with the meta-analytic prior
- Describe and plot the estimates. Evaluate model quality

```{r}
m3 <-map2stan(
  alist(
    PitchSD.m.c ~ dnorm(mu, sigma),
    mu <- alpha + beta * diagnosis,
    alpha ~ dnorm (0,1), 
    beta ~ dnorm (-0.55, 0.26),
    sigma ~ dcauchy(0,2) 
  ), 
  data=subset)
precis(m3)

#dens plot on simulated data
#Simulating data 
sim.m3 = sim(m3, data = subset, n = 1e4)

#dens plot simulated data from model on raw data
dens(sim.m2, col = "red", xlim = c(-5, 5), ylim = c(0,1),)
par(new=TRUE)
dens(subset$PitchSD.m.c, xlim = c(-5, 5), ylim = c(0,1),)
title("Model 3")
```

Step 5: Compare the models
- Plot priors and posteriors of the diagnosis effect in both models
- Compare posteriors between the two models
- Compare their relative distance from truth (WAIC)
- Discuss how they compare and whether any of them is best.

```{r}
#plot priors and posteriors
#m2
sample = extract.samples(m2, n = 1e4)
sample$bprior = rnorm(1e4, 0, 0.2)

type = rep(c("posterior", "prior"), each = 1e4)
value = c(t(sample$beta), t(sample$bprior))
d = data.frame(value,type)

library(ggplot2)
ggplot(d, aes(value, group=type)) + geom_density()

#m3
sample = extract.samples(m3, n = 1e4)
sample$bprior = rnorm(1e4, 0, 0.2)

type = rep(c("posterior", "prior"), each = 1e4)
value = c(t(sample$beta), t(sample$bprior))
d = data.frame(value,type)

ggplot(d, aes(value, group=type)) + geom_density()

#plot of posteriors
plot(coeftab(m2, m3))

#WAIC
c_models = compare(m2,m3)
c_models

plot(c_models, SE=TRUE, dSE=TRUE)

#How do they compare?
#Indistinghusiable, dSE 
# large SE, deviation is large. Overlapping

```

Step 6: Prepare a nice write up of the analysis and answer the questions at the top.

What are the consequences of using a meta-analytic prior?
We use a skeptial prior to avoid overfitting, is there a problem with using meta-analytic prior

Evaluate the models with conservative and meta-analytic priors. 
Discuss the effects on estimates. 
Discuss the effects on model quality. 
Discuss the role that meta-analytic priors should have in scientific practice. 
Should we systematically use them? Do they have drawbacks? 
Should we use them to complement more conservative approaches? 
How does the use of meta-analytic priors you suggest reflect the skeptical and cumulative nature of science?

Optional step 7: how skeptical should a prior be?
- Try different levels of skepticism and compare them using WAIC.

Optional step 8: Include other predictors
- Do age, gender and education improve the model?
- Should they be main effects or interactions?

Optional step 9: Bromance magic.
- explore the bromance code below including random effects (by default with weakly informative priors)
- learn how to change the prior
- explore effects of trial, age, gender, including the appropriate random slopes
- compare the models you created using WAIC and posterior predictive check (pp_check())


```{r}

brm_out <- brm(PitchSD ~ 1 + Diagnosis  +(1|ID_unique/Study), # Outcome as a function of the predictors as in lme4. 
               data=Data, # Define the data
               family=gaussian(), # Define the family. 
               iter = 5000, warmup = 2000, cores = 4)
summary(brm_out1)
plot(brm_out1)

```

